---
title: "Embeddings models in Langflow"
---

Embeddings models convert text into numerical vectors. These embeddings capture the semantic meaning of the input text, and allow LLMs to understand context.

Refer to your specific component's documentation for more information on parameters.

## **Use an embeddings model component in a flow**[**â€‹**](https://docs.langflow.org/components-embedding-models#use-an-embeddings-model-component-in-a-flow)

In this example of a document ingestion pipeline, the **OpenAI** embeddings model is connected to a vector database. The component converts the text chunks into vectors and stores them in the vector database. The vectorized data can be used to inform AI workloads like chatbots, similarity searches, and agents.

This embeddings component uses an OpenAI API key for authentication. Refer to your specific embeddings component's documentation for more information on authentication.

# image

## **AI/ML**

This component generates embeddings using the [AI/ML API](https://docs.aimlapi.com/api-overview/embeddings).

<Accordion title="Parameters">
  
</Accordion>

## **Amazon Bedrock Embeddings**

This component is used to load embedding models from [Amazon Bedrock](https://aws.amazon.com/bedrock/).

<Accordion title="Parameters">
  
</Accordion>

## **Astra DB vectorize**

<Note>
  **important**

  This component is deprecated as of Langflow version 1.1.2. Instead, use the [<u>Astra DB vector store component</u>](https://docs.langflow.org/components-vector-stores#astra-db-vector-store).
</Note>

Connect this component to the **Embeddings** port of the [Astra DB vector store component](https://docs.langflow.org/components-vector-stores#astra-db-vector-store) to generate embeddings.

This component requires that your Astra DB database has a collection that uses a vectorize embedding provider integration. For more information and instructions, see [Embedding Generation](https://docs.datastax.com/en/astra-db-serverless/databases/embedding-generation.html).

<Accordion title="Parameters">
  
</Accordion>

## **Azure OpenAI Embeddings**

This component generates embeddings using Azure OpenAI models.

<Accordion title="Parameters">
  
</Accordion>

## **Cloudflare Workers AI Embeddings**

This component generates embeddings using [Cloudflare Workers AI models](https://developers.cloudflare.com/workers-ai/).

<Accordion title="Parameters">
  
</Accordion>

## **Cohere Embeddings**

This component is used to load embedding models from [Cohere](https://cohere.com/).

<Accordion title="Parameters">
  
</Accordion>

## **Embedding similarity**

This component computes selected forms of similarity between two embedding vectors.

<Accordion title="Parameters">
  
</Accordion>

## **Google generative AI embeddings**

This component connects to Google's generative AI embedding service using the GoogleGenerativeAIEmbeddings class from the `langchain-google-genai` package.

<Accordion title="Parameters">
  
</Accordion>

## **Hugging Face Embeddings**

<Note>
  **note**

  This component is deprecated as of Langflow version 1.0.18. Instead, use the [<u>Hugging Face Embeddings Inference component</u>](https://docs.langflow.org/components-embedding-models#hugging-face-embeddings-inference).
</Note>

This component loads embedding models from HuggingFace.

Use this component to generate embeddings using locally downloaded Hugging Face models. Ensure you have sufficient computational resources to run the models.

<Accordion title="Parameters">
  
</Accordion>

## **Hugging Face embeddings inference**

This component generates embeddings using [Hugging Face Inference API models](https://huggingface.co/) and requires a [Hugging Face API token](https://huggingface.co/docs/hub/security-tokens) to authenticate. Local inference models do not require an API key.

Use this component to create embeddings with Hugging Face's hosted models, or to connect to your own locally hosted models.

<Accordion title="Parameters">
  
</Accordion>

### **Connect the Hugging Face component to a local embeddings model**

To run an embeddings inference locally, see the [HuggingFace documentation](https://huggingface.co/docs/text-embeddings-inference/local_cpu).

To connect the local Hugging Face model to the **Hugging Face embeddings inference** component and use it in a flow, follow these steps:

1. Create a [Vector store RAG flow](https://docs.langflow.org/vector-store-rag). There are two embeddings models in this flow that you can replace with **Hugging Face** embeddings inference components.
2. Replace both **OpenAI** embeddings model components with **Hugging Face** model components.
3. Connect both **Hugging Face** components to the **Embeddings** ports of the **Astra DB vector store** components.
4. In the **Hugging Face** components, set the **Inference Endpoint** field to the URL of your local inference model. **The API Key field is not required for local inference.**
5. Run the flow. The local inference models generate embeddings for the input text.

## **IBM watsonx embeddings**

This component generates text using [IBM ](https://www.ibm.com/watsonx)[watsonx.ai](http://watsonx.ai) foundation models.

To use **IBM **[**watsonx.ai**](http://watsonx.ai) embeddings components, replace an embeddings component with the IBM [watsonx.ai](http://watsonx.ai) component in a flow.

An example document processing flow looks like the following:

# image

This flow loads a PDF file from local storage and splits the text into chunks.

The **IBM watsonx** embeddings component converts the text chunks into embeddings, which are then stored in a Chroma DB vector store.

The values for **API endpoint**, **Project ID**, **API key**, and **Model Name** are found in your IBM watsonx.ai deployment. For more information, see the [Langchain documentation](https://python.langchain.com/docs/integrations/text_embedding/ibm_watsonx/).

### **Default models**

The component supports several default models with the following vector dimensions:

- `sentence-transformers/all-minilm-l12-v2`: 384-dimensional embeddings
- `ibm/slate-125m-english-rtrvr-v2`: 768-dimensional embeddings
- `ibm/slate-30m-english-rtrvr-v2`: 768-dimensional embeddings
- `intfloat/multilingual-e5-large`: 1024-dimensional embeddings

The component automatically fetches and updates the list of available models from your watsonx.ai instance when you provide your API endpoint and credentials.

<Accordion title="Parameters">
  
</Accordion>

## **LM Studio Embeddings**

This component generates embeddings using [LM Studio](https://lmstudio.ai/docs) models.

<Accordion title="Parameters">
  
</Accordion>

## **MistralAI**

This component generates embeddings using [MistralAI](https://docs.mistral.ai/) models.

<Accordion title="Parameters">
  
</Accordion>

## **NVIDIA**

This component generates embeddings using [NVIDIA models](https://docs.nvidia.com/).

<Accordion title="Parameters">
  
</Accordion>

## **Ollama embeddings**

This component generates embeddings using [Ollama models](https://ollama.com/).

For a list of Ollama embeddings models, see the [Ollama documentation](https://ollama.com/search?c=embedding).

To use this component in a flow, connect Langflow to your locally running Ollama server and select an embeddings model.

1. In the Ollama component, in the **Ollama Base URL** field, enter the address for your locally running Ollama server. This value is set as the `OLLAMA_HOST` environment variable in Ollama. The default base URL is `http://localhost:11434`.
2. To refresh the server's list of models, click **Refresh**.
3. In the **Ollama Model** field, select an embeddings model. This example uses `all-minilm:latest`.
4. Connect the **Ollama** embeddings component to a flow. For example, this flow connects a local Ollama server running a `all-minilm:latest` embeddings model to a [Chroma DB](https://docs.langflow.org/components-vector-stores#chroma-db) vector store to generate embeddings for split text.

# image

For more information, see the [Ollama documentation](https://ollama.com/).

<Accordion title="Parameters">
  
</Accordion>

## **OpenAI Embeddings**

This component is used to load embedding models from [OpenAI](https://openai.com/).

<Accordion title="Parameters">
  
</Accordion>

## **Text embedder**

This component generates embeddings for a given message using a specified embedding model.

<Accordion title="Parameters">
  
</Accordion>

## **VertexAI Embeddings**

This component is a wrapper around [Google Vertex AI](https://cloud.google.com/vertex-ai) [Embeddings API](https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-text-embeddings).

<Accordion title="Parameters">
  
</Accordion>