---
title: "Integrate Cleanlab Evaluations with Langflow"
---

Unlock trustworthy Agentic, RAG, and LLM pipelines with Cleanlab's evaluation and remediation suite.

[Cleanlab](https://www.cleanlab.ai/) adds automation and trust to every data point going in and every prediction coming out of AI and RAG solutions.

This Langflow integration provides three Langflow components that assess and improve the trustworthiness of any LLM or RAG pipeline output.

Use the components in this bundle to quantify the trustworthiness of any LLM response with a score between `0` and `1`, and explain why a response may be good or bad. For RAG/Agentic pipelines with context, you can evaluate context sufficiency, groundedness, helpfulness, and query clarity with quantitative scores. Additionally, you can remediate low-trust responses with warnings or fallback answers.

## **Prerequisites**[**​**](https://docs.langflow.org/integrations-cleanlab#prerequisites)

- [A Cleanlab API key](https://tlm.cleanlab.ai/)

## **CleanlabEvaluator**[**​**](https://docs.langflow.org/integrations-cleanlab#cleanlabevaluator)

This component evaluates and explains the trustworthiness of a prompt and response pair using Cleanlab. For more information on how the score works, see the [Cleanlab documentation](https://help.cleanlab.ai/tlm/).

<Accordion title="Parameters">
  
</Accordion>

## **CleanlabRemediator**[**​**](https://docs.langflow.org/integrations-cleanlab#cleanlabremediator)

This component uses the trust score from the [CleanlabEvaluator](https://docs.langflow.org/integrations-cleanlab#cleanlabevaluator) component to determine whether to show, warn about, or replace an LLM response. This component has configurables for the score threshold, warning text, and fallback message that you can customize as needed.

<Accordion title="Parameters">
  
</Accordion>

## **CleanlabRAGEvaluator**[**​**](https://docs.langflow.org/integrations-cleanlab#cleanlabragevaluator)

This component evaluates RAG and LLM pipeline outputs for trustworthiness, context sufficiency, response groundedness, helpfulness, and query ease. Learn more about Cleanlab's evaluation metrics [here](https://help.cleanlab.ai/tlm/use-cases/tlm_rag/).

Additionally, use the [CleanlabRemediator](https://docs.langflow.org/integrations-cleanlab#cleanlabremediator) component with this component to remediate low-trust responses coming from the RAG pipeline.

<Accordion title="Parameters">
  
</Accordion>

## **Cleanlab component example flows**[**​**](https://docs.langflow.org/integrations-cleanlab#cleanlab-component-example-flows)

The following example flows show how to use the **CleanlabEvaluator** and **CleanlabRemediator** components to evaluate and remediate responses from any LLM, and how to use the `CleanlabRAGEvaluator` component to evaluate RAG pipeline outputs.

### **Evaluate and remediate responses from an LLM**[**​**](https://docs.langflow.org/integrations-cleanlab#evaluate-and-remediate-responses-from-an-llm)

<Tip>
  **tip**

  Optionally, [<u>Download</u>](https://docs.langflow.org/assets/files/eval_and_remediate_cleanlab-aac380677836d5237ad3db4405aae6f7.json) the Evaluate and Remediate flow and follow along.
</Tip>

This flow evaluates and remediates the trustworthiness of a response from any LLM using the **CleanlabEvaluator** and **CleanlabRemediator** components.

# image

Connect the `Message` output from any LLM component to the `response` input of the **CleanlabEvaluator** component, and then connect the Prompt component to its `prompt` input.

The **CleanlabEvaluator** component returns a trust score and explanation from the flow.

The **CleanlabRemediator** component uses this trust score to determine whether to output the original response, warn about it, or replace it with a fallback answer.

This example shows a response that was determined to be untrustworthy (a score of `.09`) and flagged with a warning by the **CleanlabRemediator** component.

# image

To hide untrustworthy responses, configure the **CleanlabRemediator** component to replace the response with a fallback message.

# image

### **Evaluate RAG pipeline**[**​**](https://docs.langflow.org/integrations-cleanlab#evaluate-rag-pipeline)

This example flow includes the [Vector Store RAG](https://docs.langflow.org/vector-store-rag) template with the **CleanlabRAGEvaluator** component added to evaluate the flow's context, query, and response.

To use the **CleanlabRAGEvaluator** component in a flow, connect the `context`, `query`, and `response` outputs from any RAG pipeline to the **CleanlabRAGEvaluator** component.

# image

Here is an example of the `Evaluation Summary` output from the **CleanlabRAGEvaluator** component.

# image

The `Evaluation Summary` includes the query, context, response, and all evaluation results. In this example, the `Context Sufficiency` and `Response Groundedness` scores are low (a score of `0.002`) because the context doesn't contain information about the query, and the response is not grounded in the context.